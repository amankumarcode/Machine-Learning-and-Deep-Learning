{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import random"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Loading Text Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files):\n",
    "    data, sent = [], []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as rf:\n",
    "            for line in rf:\n",
    "                if line.strip() != '':\n",
    "                    # Note: the shared corpus is already tokenized\n",
    "                    sent.append(line.strip().split('\\t'))\n",
    "                else:\n",
    "                    if len(sent) > 0:\n",
    "                        data.append(sent)\n",
    "                        sent = []\n",
    "    return data\n",
    "\n",
    "sents = load_data(['FB_HI_EN_CR.txt', 'TWT_HI_EN_CR.txt', 'WA_HI_EN_CR.txt',\"WA_TE_EN_CR.txt\", \"FB_BN_EN_CR.txt\",\"FB_TE_EN_CR.txt\" ,\"TWT_BN_EN_CR.txt\" ,\"TWT_TE_EN_CR.txt\" ,\"WA_BN_EN_CR.txt\" ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating a train and validation split. Here, we are first randomly shuffling the entire corpus and use the first 80% of the instances as train and the remaining as validation. Random seed is set to allow for reproduction of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train sentences: 4186\n",
      "# Validation sentences: 1047\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(sents)\n",
    "train_sents = sents[:int(0.8*len(sents))]\n",
    "valid_sents = sents[int(0.8*len(sents)):]\n",
    "print(\"# Train sentences: %d\" % (len(train_sents)))\n",
    "print(\"# Validation sentences: %d\" % (len(valid_sents)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXAMPLE OF A TRAIN DATA POINT FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@nandi_harika', 'univ', '@'], ['elanti', 'te', 'G_R'], ['innovative', 'en', 'G_J'], ['ideas', 'univ', 'G_N'], ['ala', 'te', 'G_X'], ['vastai', 'en', 'G_X'], ['asalu', 'te', 'G_X'], ['nvu', 'univ', 'G_X'], ['super', 'en', 'G_R'], ['machi', 'univ', 'G_X'], [':)', 'univ', 'E']]\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we extract some features from the raw text data to pass onto the CRF model:\n",
    "\n",
    "1)Current word\n",
    "2)Character n-grams of the current word\n",
    "3)Context\n",
    "  -Previous word\n",
    "  -Following word\n",
    "4)Begin and End of sentence markers (BOS and EOS)\n",
    "5)Word Related features like Uppercase, Title,digits, language of the word\n",
    "6)Normalised words\n",
    "7)Vowel Count in a word\n",
    "8)Special Characters identification features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, k):\n",
    "    word = sent[k][0]\n",
    "    lang = sent[k][1]\n",
    "    wordClean = ''.join([ch for ch in word if ch in 'asdfghjklqwertyuiopzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM']).lower()\n",
    "    normalizedWord = wordClean.lower()\n",
    "    anyCap = any(char.isupper() for char in word)\n",
    "    allCap = all(char.isupper() for char in word)\n",
    "    hasSpecial = any(ord(char) > 32 and ord(char) < 65 for char in word)\n",
    "    hashTag = word[0] == '#'\n",
    "    mention = word[0] == '@'\n",
    "    features = [\n",
    "        'token=%s' % (word),\n",
    "        'lang=%s' % (lang),                                              \n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'word.lower.normalized='+ normalizedWord,\n",
    "        'anyCap=%s' %anyCap,\n",
    "        'allCap=%s' %allCap,\n",
    "        'hasSpecial=%s' %hasSpecial,\n",
    "        'hasTag=%s' % hashTag,\n",
    "        'mention=%s'% mention,\n",
    "        'vowelPercentage=%s' %vowelPercentage(word),\n",
    "    ]\n",
    "    for i in range(1,10):\n",
    "        # if the value of n is greater than the word length, we exit the loop\n",
    "        if i > len(word):\n",
    "            break\n",
    "        character_features = [word[j:j+i] for j in range(len(word)-i+1)]\n",
    "        features.extend([\n",
    "            # is count of individual n-grams important? is the order important?\n",
    "            \"char-%d-gram=%s\" % (i, ' '.join(list(set(character_features))))\n",
    "        ])\n",
    "    if k == 0:\n",
    "            \n",
    "        # first word in the sentence\n",
    "        features.append('BOS')\n",
    "    else:\n",
    "        if k == len(sent)-1:\n",
    "            features.extend([\"-1:word=%s\" % (sent[k-1][0])])\n",
    "            features.extend([\"-1:lang=%s\" % (sent[k-1][1])])\n",
    "            features.extend([\"-1:tag=%s\" % (sent[k-1][2])])\n",
    "            features.extend(['previous_word.lower=' + sent[k-1][0].lower()])\n",
    "            features.extend(['word-1.isupper=%s' % sent[k-1][0].isupper()])\n",
    "            features.extend(['word-1.istitle=%s' % sent[k-1][0].istitle()])\n",
    "            features.extend(['word-1.isdigit=%s' % sent[k-1][0].isdigit()])\n",
    "            features.extend(['word-1.vowel=%s' % vowelPercentage(sent[k-1][0])])      \n",
    "        else:\n",
    "            features.extend(['word+1.isupper=%s' % sent[k+1][0].isupper()])\n",
    "            features.extend(['ahead_word.lower=' + sent[k+1][0].lower()])\n",
    "            features.extend(['word+1.istitle=%s' % sent[k+1][0].istitle()])\n",
    "            features.extend(['word+1.isdigit=%s' % sent[k+1][0].isdigit()])\n",
    "            features.extend(['word+1.vowel=%s' % vowelPercentage(sent[k+1][0])])\n",
    "            features.extend(['word-1.vowel=%s' % vowelPercentage(sent[k-1][0])])\n",
    "            features.extend(['word-1.isupper=%s' % sent[k-1][0].isupper()])\n",
    "            features.extend(['previous_word.lower=' + sent[k-1][0].lower()])\n",
    "            features.extend(['word-1.istitle=%s' % sent[k-1][0].istitle()])\n",
    "            features.extend(['word-1.isdigit=%s' % sent[k-1][0].isdigit()])\n",
    "            features.extend([\"+1:word=%s\" % (sent[k+1][0])])\n",
    "            features.extend([\"+1:lang=%s\" % (sent[k+1][1])])\n",
    "            features.extend([\"+1:tag=%s\" % (sent[k+1][2])])\n",
    "            features.extend([\"-1:tag=%s\" % (sent[k-1][2])])\n",
    "            features.extend([\"-1:lang=%s\" % (sent[k-1][1])])\n",
    "            features.extend([\"-1:word=%s\" % (sent[k-1][0])])           \n",
    "            \n",
    "    if i == len(sent):\n",
    "        # last word in the sentence         \n",
    "        features.append('EOS')\n",
    " \n",
    "    return features\n",
    "        \n",
    "def sent2features(sent):\n",
    "    # generating features for all the words/tokens in a sentence `sent`    \n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2langs(sent):\n",
    "    return [language_label for token, language_label, pos_tag in sent]\n",
    "\n",
    "def sent2pos(sent):\n",
    "    return [pos_tag for token, language_label, pos_tag in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, language_label, pos_tag in sent]\n",
    "\n",
    "def vowelPercentage(s):\n",
    "    vowels = \"aeiou\"\n",
    "    count = 0.\n",
    "    for char in s:\n",
    "        if char in vowels:\n",
    "            count += 1\n",
    "    return (count/len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 144 ms, total: 2.39 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(sent) for sent in train_sents]\n",
    "# for training a pos-tagging system\n",
    "y_train = [sent2pos(sent) for sent in train_sents]\n",
    "\n",
    "X_test = [sent2features(sent) for sent in valid_sents]\n",
    "y_test = [sent2pos(sent) for sent in valid_sents]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "View a sample of train data with their corresponding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['token=@nandi_harika', 'lang=univ', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=nandiharika', 'anyCap=False', 'allCap=False', 'hasSpecial=True', 'hasTag=False', 'mention=True', 'vowelPercentage=0.38461538461538464', 'char-1-gram=h r a _ k d @ n i', 'char-2-gram=ha _h di ri @n an na nd i_ ka ar ik', 'char-3-gram=ndi _ha nan and rik i_h ika di_ ari @na har', 'char-4-gram=rika hari ndi_ @nan andi i_ha arik _har nand di_h', 'char-5-gram=_hari di_ha @nand andi_ harik arika nandi i_har ndi_h', 'char-6-gram=i_hari @nandi harika andi_h nandi_ di_har ndi_ha _harik', 'char-7-gram=ndi_har di_hari nandi_h i_harik @nandi_ _harika andi_ha', 'char-8-gram=andi_har di_harik nandi_ha ndi_hari i_harika @nandi_h', 'char-9-gram=andi_hari ndi_harik @nandi_ha nandi_har di_harika', 'BOS'], ['token=elanti', 'lang=te', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=elanti', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.5', 'char-1-gram=e t a l n i', 'char-2-gram=ti an nt la el', 'char-3-gram=ela nti ant lan', 'char-4-gram=anti elan lant', 'char-5-gram=elant lanti', 'char-6-gram=elanti', 'word+1.isupper=False', 'ahead_word.lower=innovative', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.5', 'word-1.vowel=0.38461538461538464', 'word-1.isupper=False', 'previous_word.lower=@nandi_harika', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=innovative', '+1:lang=en', '+1:tag=G_J', '-1:tag=@', '-1:lang=univ', '-1:word=@nandi_harika'], ['token=innovative', 'lang=en', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=innovative', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.5', 'char-1-gram=v e t a o n i', 'char-2-gram=ov nn ti no iv at ve in va', 'char-3-gram=vat inn ive ati ova tiv nov nno', 'char-4-gram=inno ovat ativ tive nova nnov vati', 'char-5-gram=innov novat ative vativ nnova ovati', 'char-6-gram=ovativ vative nnovat novati innova', 'char-7-gram=innovat ovative novativ nnovati', 'char-8-gram=novative innovati nnovativ', 'char-9-gram=innovativ nnovative', 'word+1.isupper=False', 'ahead_word.lower=ideas', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.6', 'word-1.vowel=0.5', 'word-1.isupper=False', 'previous_word.lower=elanti', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=ideas', '+1:lang=univ', '+1:tag=G_N', '-1:tag=G_R', '-1:lang=te', '-1:word=elanti'], ['token=ideas', 'lang=univ', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=ideas', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.6', 'char-1-gram=e a d s i', 'char-2-gram=as de ea id', 'char-3-gram=eas ide dea', 'char-4-gram=idea deas', 'char-5-gram=ideas', 'word+1.isupper=False', 'ahead_word.lower=ala', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.6666666666666666', 'word-1.vowel=0.5', 'word-1.isupper=False', 'previous_word.lower=innovative', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=ala', '+1:lang=te', '+1:tag=G_X', '-1:tag=G_J', '-1:lang=en', '-1:word=innovative'], ['token=ala', 'lang=te', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=ala', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.6666666666666666', 'char-1-gram=l a', 'char-2-gram=la al', 'char-3-gram=ala', 'word+1.isupper=False', 'ahead_word.lower=vastai', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.5', 'word-1.vowel=0.6', 'word-1.isupper=False', 'previous_word.lower=ideas', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=vastai', '+1:lang=en', '+1:tag=G_X', '-1:tag=G_N', '-1:lang=univ', '-1:word=ideas'], ['token=vastai', 'lang=en', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=vastai', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.5', 'char-1-gram=v t a s i', 'char-2-gram=ta as ai st va', 'char-3-gram=ast tai sta vas', 'char-4-gram=stai vast asta', 'char-5-gram=astai vasta', 'char-6-gram=vastai', 'word+1.isupper=False', 'ahead_word.lower=asalu', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.6', 'word-1.vowel=0.6666666666666666', 'word-1.isupper=False', 'previous_word.lower=ala', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=asalu', '+1:lang=te', '+1:tag=G_X', '-1:tag=G_X', '-1:lang=te', '-1:word=ala'], ['token=asalu', 'lang=te', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=asalu', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.6', 'char-1-gram=u s l a', 'char-2-gram=sa lu as al', 'char-3-gram=asa alu sal', 'char-4-gram=salu asal', 'char-5-gram=asalu', 'word+1.isupper=False', 'ahead_word.lower=nvu', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.3333333333333333', 'word-1.vowel=0.5', 'word-1.isupper=False', 'previous_word.lower=vastai', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=nvu', '+1:lang=univ', '+1:tag=G_X', '-1:tag=G_X', '-1:lang=en', '-1:word=vastai'], ['token=nvu', 'lang=univ', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=nvu', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.3333333333333333', 'char-1-gram=v n u', 'char-2-gram=nv vu', 'char-3-gram=nvu', 'word+1.isupper=False', 'ahead_word.lower=super', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.4', 'word-1.vowel=0.6', 'word-1.isupper=False', 'previous_word.lower=asalu', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=super', '+1:lang=en', '+1:tag=G_R', '-1:tag=G_X', '-1:lang=te', '-1:word=asalu'], ['token=super', 'lang=en', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=super', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.4', 'char-1-gram=e u r s p', 'char-2-gram=er su up pe', 'char-3-gram=sup per upe', 'char-4-gram=uper supe', 'char-5-gram=super', 'word+1.isupper=False', 'ahead_word.lower=machi', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.4', 'word-1.vowel=0.3333333333333333', 'word-1.isupper=False', 'previous_word.lower=nvu', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=machi', '+1:lang=univ', '+1:tag=G_X', '-1:tag=G_X', '-1:lang=univ', '-1:word=nvu'], ['token=machi', 'lang=univ', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=machi', 'anyCap=False', 'allCap=False', 'hasSpecial=False', 'hasTag=False', 'mention=False', 'vowelPercentage=0.4', 'char-1-gram=h a m c i', 'char-2-gram=ac hi ma ch', 'char-3-gram=chi mac ach', 'char-4-gram=mach achi', 'char-5-gram=machi', 'word+1.isupper=False', 'ahead_word.lower=:)', 'word+1.istitle=False', 'word+1.isdigit=False', 'word+1.vowel=0.0', 'word-1.vowel=0.4', 'word-1.isupper=False', 'previous_word.lower=super', 'word-1.istitle=False', 'word-1.isdigit=False', '+1:word=:)', '+1:lang=univ', '+1:tag=E', '-1:tag=G_R', '-1:lang=en', '-1:word=super'], ['token=:)', 'lang=univ', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'word.lower.normalized=', 'anyCap=False', 'allCap=False', 'hasSpecial=True', 'hasTag=False', 'mention=False', 'vowelPercentage=0.0', 'char-1-gram=) :', 'char-2-gram=:)', '-1:word=machi', '-1:lang=univ', '-1:tag=G_X', 'previous_word.lower=machi', 'word-1.isupper=False', 'word-1.istitle=False', 'word-1.isdigit=False', 'word-1.vowel=0.4']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Preparing the CRF model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 0 ns, total: 1.69 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1e-4,   # coefficient for L1 penalty\n",
    "    'c2': 0.5,  # coefficient for L2 penalty\n",
    "    'max_iterations': 300,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 44 ms, total: 1min 6s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('POS_model')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating a tagger and loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f49de7efe10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('POS_model')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Making predictions on the validation data using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ANI_news tumhari party hamesha note hi toh leti hai , bahut saare note . ðŸ’µ\n",
      "\n",
      "Predicted: @ G_N G_N G_N G_N G_N G_PRP G_V G_V G_X G_N G_N G_N G_X G_X\n",
      "Correct:   @ G_PRP G_N G_R G_N G_PRT CC G_N G_V G_X G_SYM G_SYM G_N G_X G_X\n"
     ]
    }
   ],
   "source": [
    "example_sent = valid_sents[5]\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2pos(example_sent)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function to take out all the POS tags in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        for y in x:\n",
    "            if y not in unique_list: \n",
    "                unique_list.append(y) \n",
    "    # print list \n",
    "    #for x in unique_list: \n",
    "       # print(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All POS labels present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@': 0,\n",
       " 'G_R': 1,\n",
       " 'G_J': 2,\n",
       " 'G_N': 3,\n",
       " 'G_X': 4,\n",
       " 'E': 5,\n",
       " 'G_V': 6,\n",
       " '$': 7,\n",
       " 'G_PRT': 8,\n",
       " 'G_PRP': 9,\n",
       " 'CC': 10,\n",
       " 'PSP': 11,\n",
       " 'DT': 12,\n",
       " 'G_SYM': 13,\n",
       " '#': 14,\n",
       " 'U': 15,\n",
       " '~': 16,\n",
       " 'null': 17}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {}\n",
    "for i,j in enumerate(unique(y_train)):\n",
    "    labels.update({j:i})\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'G_R',\n",
       " 'G_J',\n",
       " 'G_N',\n",
       " 'G_X',\n",
       " 'E',\n",
       " 'G_V',\n",
       " '$',\n",
       " 'G_PRT',\n",
       " 'G_PRP',\n",
       " 'CC',\n",
       " 'PSP',\n",
       " 'DT',\n",
       " 'G_SYM',\n",
       " '#',\n",
       " 'U',\n",
       " '~',\n",
       " 'null']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq = unique(y_train)\n",
    "uniq"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classification Report of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           @       0.95      1.00      0.98       442\n",
      "         G_R       0.84      0.67      0.75       501\n",
      "         G_J       0.81      0.63      0.71       854\n",
      "         G_N       0.75      0.89      0.82      5276\n",
      "         G_X       0.87      0.86      0.87      2824\n",
      "           E       0.96      0.86      0.91       175\n",
      "         G_V       0.80      0.73      0.76      2296\n",
      "           $       0.75      0.69      0.72       164\n",
      "       G_PRT       0.69      0.52      0.60       494\n",
      "       G_PRP       0.82      0.76      0.79      1175\n",
      "          CC       0.77      0.68      0.72       314\n",
      "         PSP       0.76      0.73      0.74       938\n",
      "          DT       0.86      0.86      0.86       497\n",
      "       G_SYM       0.74      0.42      0.54       140\n",
      "           #       0.87      0.94      0.90       175\n",
      "           U       0.95      0.84      0.89       133\n",
      "           ~       0.75      0.60      0.67        20\n",
      "        null       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.80     16474\n",
      "   macro avg       0.77      0.70      0.73     16474\n",
      "weighted avg       0.80      0.80      0.80     16474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in y_pred for tag in row])\n",
    "truths = np.array([labels[tag] for row in y_test for tag in row])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=uniq))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating a prediction and true array for calculating accuracy and getting the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.array([tag for row in y_pred for tag in row])\n",
    "true = np.array([tag for row in y_test for tag in row])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ACCURACY OF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8012018938934078"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "accuracy = 0\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] == true[i]:\n",
    "        count+=1\n",
    "accuracy = count/len(true)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 441    0    0    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   0  338   16   76    8    0   27    0   10    4    0   13    3    4\n",
      "     2    0    0    0]\n",
      " [   1   19  534  237    4    0   41    3    3    4    1    2    5    0\n",
      "     0    0    0    0]\n",
      " [  15    7   62 4695  144    2  231    1   23   39    0   27   13    7\n",
      "     6    3    0    1]\n",
      " [   1    4    5  276 2428    5   30    1   10   27    0   21    7    0\n",
      "     1    1    4    3]\n",
      " [   0    1    0    6   13  151    1    1    0    1    0    0    1    0\n",
      "     0    0    0    0]\n",
      " [   1   11   15  490   58    0 1680    0   11   13    0   14    1    0\n",
      "     2    0    0    0]\n",
      " [   0    0    0   16   14    0    1  113    1    0    0    1    0    1\n",
      "    13    0    0    4]\n",
      " [   1    7    4   98   20    0   23    0  258   14   10   51    2    3\n",
      "     1    1    0    1]\n",
      " [   0    6    6  129   29    0   35    0   19  895    3   23   26    3\n",
      "     0    1    0    0]\n",
      " [   1    0    1   16    6    0    5    0    9    1  212   57    4    2\n",
      "     0    0    0    0]\n",
      " [   1    7    3   66   17    0   22    2   22   64   47  682    4    1\n",
      "     0    0    0    0]\n",
      " [   0    0    7   28    5    0    6    0    0   23    2    1  425    0\n",
      "     0    0    0    0]\n",
      " [   0    0    7   44    3    0    2   10    5    4    1    1    4   59\n",
      "     0    0    0    0]\n",
      " [   0    0    0   11    0    0    0    0    0    0    0    0    0    0\n",
      "   164    0    0    0]\n",
      " [   0    0    0   17    2    0    1    0    1    0    0    0    0    0\n",
      "     0  112    0    0]\n",
      " [   0    0    0    0    8    0    0    0    0    0    0    0    0    0\n",
      "     0    0   12    0]\n",
      " [   0    0    0   14   22    0    0   20    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true, predict ,labels=uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
